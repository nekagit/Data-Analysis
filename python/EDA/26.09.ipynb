{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# WELCOME TO PYTHON COURSE (26.09)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Results NUMPY (-> 10 Uhr)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Task 1: Create a 2D array representing a 5x5 grid with random integers\n",
    "grid_5x5 = np.random.randint(1, 100, size=(5, 5))\n",
    "print(\"Task 1: 5x5 grid with random integers\\n\", grid_5x5)\n",
    "\n",
    "# Task 2: Reshape a 1D array of size 9 into a 3x3 matrix\n",
    "array_1d = np.arange(9)\n",
    "matrix_3x3 = array_1d.reshape((3, 3))\n",
    "print(\"\\nTask 2: 3x3 matrix\\n\", matrix_3x3)\n",
    "\n",
    "# Task 3: Create a 3D array and access a specific element\n",
    "array_3d = np.random.randint(1, 100, size=(3, 3, 3))\n",
    "print(\"\\nTask 3: Element in 3D array\\n\", array_3d[1, 2, 0])\n",
    "\n",
    "# Task 4: Apply mathematical functions on an array\n",
    "array = np.arange(10)\n",
    "sqrt_array = np.sqrt(array)\n",
    "exp_array = np.exp(array)\n",
    "sin_array = np.sin(array)\n",
    "print(\"\\nTask 4: Mathematical operations\\nSquare Root:\", sqrt_array)\n",
    "print(\"Exponential:\", exp_array)\n",
    "print(\"Sine:\", sin_array)\n",
    "\n",
    "# Task 5: Compute statistics (mean, median, std)\n",
    "random_data = np.random.rand(100)\n",
    "mean_value = np.mean(random_data)\n",
    "median_value = np.median(random_data)\n",
    "std_value = np.std(random_data)\n",
    "print(\"\\nTask 5: Mean:\", mean_value, \"Median:\", median_value, \"Std:\", std_value)\n",
    "\n",
    "# Task 6: Element-wise operations between arrays\n",
    "array_a = np.array([1, 2, 3])\n",
    "array_b = np.array([4, 5, 6])\n",
    "sum_array = array_a + array_b\n",
    "product_array = array_a * array_b\n",
    "print(\"\\nTask 6: Element-wise sum and product\\nSum:\", sum_array, \"Product:\", product_array)\n",
    "\n",
    "# Task 7: Broadcasting example\n",
    "matrix = np.ones((3, 3))\n",
    "vector = np.array([1, 2, 3])\n",
    "broadcast_result = matrix + vector\n",
    "print(\"\\nTask 7: Broadcasting result\\n\", broadcast_result)\n",
    "\n",
    "# Task 8: Multiply matrices using broadcasting\n",
    "matrix_4x1 = np.random.rand(4, 1)\n",
    "matrix_1x4 = np.random.rand(1, 4)\n",
    "broadcast_mult = matrix_4x1 * matrix_1x4\n",
    "print(\"\\nTask 8: Broadcasting multiplication result\\n\", broadcast_mult)\n",
    "\n",
    "# Task 9: Subtract the mean from each column\n",
    "matrix = np.random.rand(4, 3)\n",
    "mean_subtracted = matrix - matrix.mean(axis=0)\n",
    "print(\"\\nTask 9: Mean-subtracted matrix\\n\", mean_subtracted)\n",
    "\n",
    "# Task 10: Matrix multiplication\n",
    "matrix_a = np.random.rand(3, 3)\n",
    "matrix_b = np.random.rand(3, 3)\n",
    "matrix_product = np.dot(matrix_a, matrix_b)\n",
    "print(\"\\nTask 10: Matrix product\\n\", matrix_product)\n",
    "\n",
    "# Task 11: Determinant and inverse of a matrix\n",
    "matrix_c = np.random.rand(3, 3)\n",
    "det = np.linalg.det(matrix_c)\n",
    "inv = np.linalg.inv(matrix_c)\n",
    "print(\"\\nTask 11: Determinant:\", det, \"Inverse:\\n\", inv)\n",
    "\n",
    "# Task 12: Solve a system of linear equations\n",
    "coefficients = np.array([[2, -1], [-1, 2]])\n",
    "constants = np.array([1, 0])\n",
    "solution = np.linalg.solve(coefficients, constants)\n",
    "print(\"\\nTask 12: Solution to linear equations\\n\", solution)\n",
    "\n",
    "# Task 13: Eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrix_c)\n",
    "print(\"\\nTask 13: Eigenvalues\\n\", eigenvalues)\n",
    "print(\"Eigenvectors\\n\", eigenvectors)\n",
    "\n",
    "# Task 14: Generate random integers\n",
    "random_integers = np.random.randint(50, 100, size=10)\n",
    "print(\"\\nTask 14: Random integers\\n\", random_integers)\n",
    "\n",
    "# Task 15: Simulate rolling a dice\n",
    "dice_rolls = np.random.randint(1, 7, size=100)\n",
    "print(\"\\nTask 15: Dice rolls simulation\\n\", dice_rolls)\n",
    "\n",
    "# Task 16: Create random numbers from a normal distribution and visualize\n",
    "normal_data = np.random.randn(1000)\n",
    "plt.hist(normal_data, bins=30)\n",
    "plt.title(\"Task 16: Histogram of normal distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Task 17: Shuffle an array and take a random sample\n",
    "array = np.arange(10)\n",
    "np.random.shuffle(array)\n",
    "sample = np.random.choice(array, size=5, replace=False)\n",
    "print(\"\\nTask 17: Shuffled array:\", array, \"Random sample:\", sample)\n",
    "\n",
    "# Task 18: Create a Pandas DataFrame using NumPy array\n",
    "df = pd.DataFrame(np.random.rand(5, 3), columns=[\"A\", \"B\", \"C\"])\n",
    "print(\"\\nTask 18: Pandas DataFrame\\n\", df)\n",
    "\n",
    "# Task 19: Plot using NumPy and Matplotlib\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Task 19: Sine Wave Plot\")\n",
    "plt.show()\n",
    "\n",
    "# Task 20: Import a CSV into Pandas, convert a column to NumPy array\n",
    "# df = pd.read_csv('data.csv')  # Example with real data\n",
    "# column_as_array = df['column_name'].values\n",
    "\n",
    "# Task 21: Compare Python loop vs NumPy sum\n",
    "python_list = list(range(1000000))\n",
    "start_time = time.time()\n",
    "python_sum = sum(python_list)\n",
    "python_time = time.time() - start_time\n",
    "\n",
    "numpy_array = np.array(python_list)\n",
    "start_time = time.time()\n",
    "numpy_sum = np.sum(numpy_array)\n",
    "numpy_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTask 21: Python sum time: {python_time:.6f}s, NumPy sum time: {numpy_time:.6f}s\")\n",
    "\n",
    "# Task 22: Vectorized vs loop operations\n",
    "array = np.arange(1000000)\n",
    "start_time = time.time()\n",
    "loop_result = [i ** 2 for i in array]\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "vectorized_result = array ** 2\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTask 22: Loop time: {loop_time:.6f}s, Vectorized time: {vectorized_time:.6f}s\")\n",
    "\n",
    "# Task 23: Process weather data (example task with real dataset)\n",
    "# weather_data = pd.read_csv('weather.csv')  # Example with real data\n",
    "# weather_np = weather_data[['Temperature', 'Humidity']].to_numpy()\n",
    "# mean_temp = np.mean(weather_np[:, 0])\n",
    "\n",
    "# Task 24: Filter an array based on a threshold\n",
    "array = np.random.rand(10)\n",
    "filtered_array = array[array > 0.5]\n",
    "print(\"\\nTask 24: Filtered array\\n\", filtered_array)\n",
    "\n",
    "# Task 25: Group data based on a condition\n",
    "grouped_data = array[array > array.mean()]\n",
    "print(\"\\nTask 25: Grouped data based on mean\\n\", grouped_data)\n",
    "\n",
    "# Task 26: Normalize dataset (features)\n",
    "dataset = np.random.rand(100, 5)\n",
    "normalized_data = (dataset - np.mean(dataset, axis=0)) / np.std(dataset, axis=0)\n",
    "print(\"\\nTask 26: Normalized data\\n\", normalized_data)\n",
    "\n",
    "# Task 27: Split dataset into training and testing sets\n",
    "train_data = dataset[:80]\n",
    "test_data = dataset[80:]\n",
    "print(\"\\nTask 27: Training and Testing sets\\nTraining data size:\", train_data.shape, \"Test data size:\", test_data.shape)\n",
    "\n",
    "# Task 28: Linear regression (manual implementation using NumPy)\n",
    "x = np.random.rand(100)\n",
    "y = 3 * x + np.random.randn(100) * 0.1  # y = 3x + noise\n",
    "A = np.vstack([x, np.ones_like(x)]).T\n",
    "slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "print(\"\\nTask 28: Linear regression slope:\", slope, \"Intercept:\", intercept)\n",
    "\n",
    "# Task 29: Simulate projectile motion\n",
    "time_points = np.linspace(0, 10, num=100)\n",
    "initial_velocity = 10\n",
    "g = 9.81\n",
    "heights = initial_velocity * time_points - 0.5 * g * time_points ** 2\n",
    "plt.plot(time_points, heights)\n",
    "plt.title(\"Task 29: Projectile motion\")\n",
    "plt.show()\n",
    "\n",
    "# Task 30: Solve differential equation (simple ODE)\n",
    "# Example placeholder for differential equation simulation\n",
    "\n",
    "# Task 31: Fourier transform of a sine wave signal\n",
    "signal = np.sin(2 * np.pi * 5 * time_points)\n",
    "fourier_transform = np.fft.fft(signal)\n",
    "frequencies = np.fft.fftfreq(len(signal), d=time_points[1] - time_points[0])\n",
    "plt.plot(frequencies, np.abs(fourier_transform))\n",
    "plt.title(\"Task 31: Fourier Transform\")\n",
    "plt.show()\n",
    "\n",
    "# Task 32: Load and manipulate an image as NumPy array\n",
    "# image = plt.imread('image.png')  # Example with real image\n",
    "# inverted_image = 255 - image\n",
    "\n",
    "# Task 33: Simulate coin tosses and calculate probability\n",
    "coin_tosses = np.random.choice(['Heads', 'Tails'], size=1000)\n",
    "heads_probability = np.sum(coin_tosses == 'Heads') / 1000\n",
    "print(\"\\nTask 33: Heads probability\\n\", heads_probability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# TEST NUMPY (10.15 -> 11 Uhr) \n",
    "\n",
    "---\n",
    "#### **Question 1:**\n",
    "\n",
    "Given a 3D NumPy array `arr` of shape `(4, 5, 6)`, how would you swap the second and third axes of the array?\n",
    "\n",
    "- a) `arr.transpose(2, 0, 1)`\n",
    "- b) `arr.swapaxes(1, 2)`\n",
    "- c) `np.swap(arr, axis1=1, axis2=2)`\n",
    "- d) `arr.swap(1, 2)`\n",
    "\n",
    "#### **Question 2:**\n",
    "\n",
    "How do you find the indices of the top 3 largest values along the second axis of a 2D array `arr`?\n",
    "\n",
    "- a) `np.argsort(arr, axis=1)[:, -3:]`\n",
    "- b) `np.sort(arr, axis=1)[:, -3:]`\n",
    "- c) `arr.argsort(axis=1)[:, -3:]`\n",
    "- d) `arr.argpartition(-3, axis=1)`\n",
    "\n",
    "#### **Question 3:**\n",
    "\n",
    "You have a large NumPy array `arr` with integers, and you want to replace all negative values in `arr` with their absolute values, **without** using loops. Which method would you use?\n",
    "\n",
    "- a) `np.abs(arr)`\n",
    "- b) `np.where(arr < 0, np.abs(arr), arr)`\n",
    "- c) `arr[arr < 0] = -arr[arr < 0]`\n",
    "- d) `arr[arr < 0] *= -1`\n",
    "\n",
    "#### **Question 4:**\n",
    "\n",
    "You have two large NumPy arrays, `a` and `b`, both of shape `(1000, 1000)`. What is the most efficient way to compute the **element-wise product**?\n",
    "\n",
    "- a) `np.multiply(a, b)`\n",
    "- b) `a * b`\n",
    "- c) `np.dot(a, b)`\n",
    "- d) `np.einsum('ij,ij->ij', a, b)`\n",
    "\n",
    "#### **Question 5:**\n",
    "\n",
    "You have an array `arr` of shape `(10, 10)` and you want to **normalize** the array such that each row has a mean of `0` and a standard deviation of `1`. What is the correct approach?\n",
    "\n",
    "- a) `(arr - np.mean(arr, axis=1)) / np.std(arr, axis=1)`\n",
    "- b) `(arr - np.mean(arr, axis=1, keepdims=True)) / np.std(arr, axis=1, keepdims=True)`\n",
    "- c) `(arr - np.mean(arr, axis=0)) / np.std(arr, axis=0)`\n",
    "- d) `(arr - np.mean(arr)) / np.std(arr)`\n",
    "\n",
    "#### **Question 6:**\n",
    "\n",
    "Given a 1D array `arr` of shape `(n,)`, what is the **most efficient** way to create a 2D array where each row is a circular shift of the original array?\n",
    "\n",
    "- a) `np.array([np.roll(arr, i) for i in range(len(arr))])`\n",
    "- b) `np.fromfunction(lambda i, j: arr[(j - i) % len(arr)], (len(arr), len(arr)), dtype=int)`\n",
    "- c) `np.stack([np.roll(arr, i) for i in range(len(arr))])`\n",
    "- d) `arr.reshape(-1, 1)`\n",
    "\n",
    "#### **Question 7:**\n",
    "\n",
    "How do you perform a **matrix multiplication** between two 2D arrays `A` and `B` of shape `(m, n)` and `(n, p)` respectively, and ensure that the result uses **least memory**?\n",
    "\n",
    "- a) `np.matmul(A, B)`\n",
    "- b) `A @ B`\n",
    "- c) `np.dot(A, B)`\n",
    "- d) `np.einsum('ij,jk->ik', A, B, optimize=True)`\n",
    "\n",
    "#### **Question 8:**\n",
    "\n",
    "You have an array `arr` of shape `(6, 6)`, and you want to extract the **main diagonal** and the two **upper diagonals** above the main diagonal. How can you do this efficiently?\n",
    "\n",
    "- a) `np.diag(arr) + np.diag(arr, k=1) + np.diag(arr, k=2)`\n",
    "- b) `np.vstack([np.diag(arr, k=i) for i in range(3)])`\n",
    "- c) `np.array([np.diag(arr, k=i) for i in range(3)])`\n",
    "- d) `np.diag(arr, k=0) + np.diag(arr, k=1) + np.diag(arr, k=2)`\n",
    "\n",
    "#### **Question 9:**\n",
    "\n",
    "You are given a very large NumPy array `arr`. How do you efficiently compute the **median** of the entire array without copying data or using excessive memory?\n",
    "\n",
    "- a) `np.median(arr)`\n",
    "- b) `np.percentile(arr, 50)`\n",
    "- c) `np.sort(arr)[len(arr)//2]`\n",
    "- d) `np.partition(arr, len(arr) // 2)[len(arr) // 2]`\n",
    "\n",
    "#### **Question 10:**\n",
    "\n",
    "You have a **structured array** with fields `'name'`, `'age'`, and `'weight'`. How would you sort this array by `'age'` first, and in the case of ties, by `'weight'`?\n",
    "\n",
    "- a) `np.sort(arr, order=['age', 'weight'])`\n",
    "- b) `arr[np.lexsort((arr['weight'], arr['age']))]`\n",
    "- c) `arr[np.argsort(arr, order=('age', 'weight'))]`\n",
    "- d) `arr[np.argsort((arr['age'], arr['weight']))]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# EDA (-> 11.15)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **Exploratory Data Analysis (EDA) Handout**\n",
    "\n",
    "### **1. Introduction to EDA**\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a critical step in the data analysis process. It involves analyzing and summarizing the main characteristics of a dataset, often using visual methods. EDA helps to uncover patterns, spot anomalies, test hypotheses, and check assumptions, making it easier to prepare the data for further analysis or modeling.\n",
    "\n",
    "### **2. Importance of EDA**\n",
    "\n",
    "- **Understanding Data**: EDA provides insights into the structure and nuances of the dataset, helping analysts understand relationships between variables.\n",
    "- **Identifying Patterns**: It reveals trends, correlations, and patterns that may not be immediately apparent.\n",
    "- **Detecting Outliers**: EDA helps identify outliers or anomalies that could skew results or indicate data quality issues.\n",
    "- **Informing Model Selection**: Insights gained from EDA guide the choice of appropriate models and techniques for further analysis.\n",
    "\n",
    "### **3. Key Techniques in EDA**\n",
    "\n",
    "#### **3.1. Descriptive Statistics**\n",
    "\n",
    "Descriptive statistics summarize the main features of a dataset. Key metrics include:\n",
    "\n",
    "- **Mean**: The average value.\n",
    "- **Median**: The middle value when the data is sorted.\n",
    "- **Mode**: The most frequently occurring value.\n",
    "- **Standard Deviation**: A measure of the amount of variation or dispersion in a set of values.\n",
    "\n",
    "#### **3.2. Data Visualization**\n",
    "\n",
    "Visualizing data helps in understanding complex relationships. Common visualization techniques include:\n",
    "\n",
    "- **Histograms**: Display the distribution of a single variable by showing the frequency of different value ranges.\n",
    "- **Box Plots**: Summarize data based on five summary statistics (minimum, first quartile, median, third quartile, maximum), highlighting outliers.\n",
    "- **Scatter Plots**: Show relationships between two quantitative variables, helping to identify correlations.\n",
    "- **Bar Charts**: Compare different categories or groups, illustrating differences in magnitude.\n",
    "\n",
    "#### **3.3. Correlation Analysis**\n",
    "\n",
    "Correlation analysis assesses the strength and direction of relationships between variables. Common metrics include:\n",
    "\n",
    "- **Pearson Correlation Coefficient**: Measures linear correlation between two continuous variables.\n",
    "- **Spearman's Rank Correlation**: Assesses how well the relationship between two variables can be described by a monotonic function.\n",
    "\n",
    "#### **3.4. Data Cleaning**\n",
    "\n",
    "During EDA, itâ€™s crucial to address issues like:\n",
    "\n",
    "- **Missing Values**: Identify and handle missing data points through imputation or removal.\n",
    "- **Outliers**: Investigate and decide whether to retain or remove outliers based on their impact on analysis.\n",
    "- **Data Types**: Ensure that data types (e.g., categorical, numerical) are appropriately assigned for analysis.\n",
    "\n",
    "### **4. EDA Tools and Libraries**\n",
    "\n",
    "Several tools and libraries are commonly used for EDA:\n",
    "\n",
    "- **Pandas**: A powerful library in Python for data manipulation and analysis, providing easy-to-use data structures.\n",
    "- **Matplotlib** and **Seaborn**: Libraries for creating static, animated, and interactive visualizations in Python.\n",
    "- **Tableau**: A popular software for creating interactive and shareable dashboards.\n",
    "- **R**: A statistical programming language with numerous packages for data analysis and visualization.\n",
    "\n",
    "### **5. EDA Process**\n",
    "\n",
    "The EDA process typically follows these steps:\n",
    "\n",
    "1. **Data Collection**: Gather data from various sources (databases, CSV files, APIs).\n",
    "2. **Data Cleaning**: Clean and preprocess the data, addressing missing values and outliers.\n",
    "3. **Descriptive Statistics**: Calculate and review summary statistics.\n",
    "4. **Data Visualization**: Create visualizations to explore the data and identify patterns.\n",
    "5. **Correlation Analysis**: Analyze relationships between variables.\n",
    "6. **Document Findings**: Summarize insights gained from the EDA process to inform further analysis.\n",
    "\n",
    "### **6. Conclusion**\n",
    "\n",
    "EDA is an essential step in data analysis that lays the foundation for deeper insights and more complex analyses. By using various techniques, analysts can understand their data better, leading to more informed decision-making and effective modeling.\n",
    "\n",
    "### **7. Further Reading and Resources**\n",
    "\n",
    "- **Books**: \"Python for Data Analysis\" by Wes McKinney, \"Data Science for Business\" by Foster Provost and Tom Fawcett.\n",
    "- **Online Courses**: Platforms like Coursera, edX, and Udacity offer courses on data analysis and EDA.\n",
    "- **Documentation**: Refer to the official documentation of libraries like Pandas, Matplotlib, and Seaborn for detailed features and capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# PREVIEW EDA (-> 12Uhr)\n",
    "\n",
    "---\n",
    "\n",
    "### **Before EDA:**\n",
    "\n",
    "1. **Problem Definition**: \n",
    "   - Clearly define the objective of the analysis or the business problem you're trying to solve. This helps guide the data analysis process.\n",
    "\n",
    "2. **Data Collection**: \n",
    "   - Gather the necessary data from various sources, such as databases, APIs, or CSV files. This step involves ensuring you have all the relevant data needed for analysis.\n",
    "\n",
    "3. **Data Understanding**: \n",
    "   - Take an initial look at the data to understand its structure, types, and basic characteristics. This includes checking for missing values, identifying data types, and performing basic descriptive analysis.\n",
    "\n",
    "### **During EDA:**\n",
    "- EDA is where you extensively explore the dataset, uncover patterns, understand data distributions, and identify any potential issues or anomalies. This process is summarized in the 10 tasks you've outlined (e.g., generating data, summarizing data, visualizing relationships, identifying outliers).\n",
    "\n",
    "### **After EDA:**\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - **Handling Missing Data**: Decide how to deal with missing values (already done partially in EDA). You might use techniques like imputation, deletion, or predictive methods.\n",
    "   - **Outlier Treatment**: Address outliers that might skew analysis or models, either by removing them or transforming the data.\n",
    "   - **Data Transformation**: Normalize or standardize data if needed, especially for features with varying scales.\n",
    "   - **Encoding Categorical Variables**: Convert categorical variables into numerical formats using techniques like one-hot encoding, label encoding, or ordinal encoding.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - **Feature Creation**: Create new features that might be more informative for your analysis or model.\n",
    "   - **Feature Selection**: Identify and retain only the most important features based on statistical analysis, correlation, or domain knowledge.\n",
    "   - **Dimensionality Reduction**: Use techniques like PCA (Principal Component Analysis) if you have high-dimensional data.\n",
    "\n",
    "3. **Data Splitting** (if building a predictive model):\n",
    "   - Split the dataset into training and testing (and possibly validation) sets to prepare for model training and evaluation.\n",
    "\n",
    "4. **Model Building and Training**:\n",
    "   - Choose an appropriate model (e.g., regression, classification, clustering) based on the problem at hand.\n",
    "   - Train the model using the training data and fine-tune hyperparameters as needed.\n",
    "\n",
    "5. **Model Evaluation**:\n",
    "   - Evaluate model performance using metrics relevant to the problem, such as accuracy, precision, recall, F1 score, RMSE, etc.\n",
    "   - Validate the model using the test/validation set to check for overfitting or underfitting.\n",
    "\n",
    "6. **Model Interpretation and Analysis**:\n",
    "   - Interpret the results, assess feature importance, and ensure that the model aligns with expectations.\n",
    "   - Use explainability techniques (e.g., SHAP, LIME) to understand model predictions.\n",
    "\n",
    "7. **Deployment and Monitoring** (if needed):\n",
    "   - Deploy the model into production or create dashboards/reports to present your findings.\n",
    "   - Monitor model performance over time to ensure it continues to deliver accurate results.\n",
    "\n",
    "8. **Documentation and Reporting**:\n",
    "   - Document your analysis, findings, and model performance to communicate insights to stakeholders.\n",
    "   - Create visualizations, dashboards, or reports summarizing the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# FINAL TASKS: FINAL STARTUP TASK\n",
    "\n",
    "# Space Tech Startup Analysis: Galactic Innovations Inc.\n",
    "\n",
    "## Scenario\n",
    "Galactic Innovations Inc. is a cutting-edge space technology startup that designs and manufactures advanced satellites, space probes, and propulsion systems. The company has been operating for five years and has seen rapid growth. However, they're facing challenges in project management, resource allocation, and market positioning. As a data analyst, you've been hired to provide insights that will help optimize their operations and guide strategic decisions.\n",
    "\n",
    "## Data Overview\n",
    "You have access to the following datasets:\n",
    "1. Employee Data: Information about 100 employees\n",
    "2. Project Data: Details of 50 space technology projects\n",
    "3. Financial Data: Monthly financial records for the past 5 years\n",
    "4. Customer Data: Information about 200 clients and their satisfaction scores\n",
    "5. Market Data: Quarterly market trends and competitor information\n",
    "\n",
    "\n",
    "\n",
    "#### **1. Data Acquisition and Integration**\n",
    "   - **Goal**: Gather all necessary datasets from different sources and integrate them into a unified data warehouse.\n",
    "\n",
    "#### **2. Data Cleaning and Preparation**\n",
    "   - **Goal**: Ensure all data is consistent, accurate, and free of errors.\n",
    "   - **Tasks**:\n",
    "     - Handle missing values, duplicates, and inconsistencies.\n",
    "     - Standardize formats (e.g., date, currency, categorical values).\n",
    "     - Validate data accuracy using cross-referencing and outlier detection.\n",
    "\n",
    "#### **3. Data Transformation and Feature Engineering**\n",
    "   - **Goal**: Create new features that provide additional insights.\n",
    "   - **Tasks**:\n",
    "     - Normalize/standardize numerical data.\n",
    "     - Encode categorical variables.\n",
    "     - Create derived metrics (e.g., revenue per employee, project efficiency scores).\n",
    "\n",
    "#### **4. Initial Data Profiling**\n",
    "   - **Goal**: Generate a preliminary understanding of the dataset's structure and quality.\n",
    "   - **Tasks**:\n",
    "     - Create summary statistics (mean, median, mode, standard deviation).\n",
    "     - Generate data distribution reports for all features.\n",
    "\n",
    "\n",
    "### Task 1: Employee Performance and Retention Analysis\n",
    "- Perform EDA on the employee dataset\n",
    "- Create a scatter plot of employee performance vs. salary using matplotlib\n",
    "- Generate a heatmap of correlation between employee metrics using seaborn\n",
    "- Develop an interactive dashboard for HR using plotly\n",
    "\n",
    "### Task 2: Project Profitability and Timeline Analysis\n",
    "- Clean and preprocess the project data\n",
    "- Create a bar chart of project durations using matplotlib\n",
    "- Generate a scatter plot of project cost vs. revenue using seaborn\n",
    "- Develop an interactive Gantt chart for project timelines using plotly\n",
    "\n",
    "### Task 3: Financial Trend Analysis\n",
    "- Perform time series analysis on the financial data\n",
    "- Create a line plot of revenue, expenses, and profit over time using matplotlib\n",
    "- Generate a stacked area chart of revenue streams using seaborn\n",
    "- Develop an interactive financial dashboard using plotly\n",
    "\n",
    "### Task 4: Customer Satisfaction and Revenue Impact\n",
    "- Analyze the relationship between customer satisfaction and revenue\n",
    "- Create a histogram of customer satisfaction scores using matplotlib\n",
    "- Generate a pair plot of customer metrics using seaborn\n",
    "- Develop an interactive scatter plot of satisfaction vs. revenue using plotly\n",
    "\n",
    "### Task 5: Market Share and Competitive Analysis\n",
    "- Analyze market trends and company's position\n",
    "- Create a pie chart of market share using matplotlib\n",
    "- Generate a grouped bar chart of competitor comparison using seaborn\n",
    "- Develop an interactive treemap of market segments using plotly\n",
    "\n",
    "### Task 6: Resource Utilization and Optimization\n",
    "- Analyze resource allocation across projects\n",
    "- Create a stacked bar chart of resource utilization using matplotlib\n",
    "- Generate a heatmap of resource efficiency using seaborn\n",
    "- Develop an interactive Sankey diagram of resource flow using plotly\n",
    "\n",
    "### Task 7: Technology Innovation Impact Analysis\n",
    "- Analyze the impact of new technologies on project success\n",
    "- Create a bubble chart of technology impact using matplotlib\n",
    "- Generate a swarm plot of innovation scores using seaborn\n",
    "- Develop an interactive 3D scatter plot of technology metrics using plotly\n",
    "\n",
    "### Task 8: Supply Chain and Logistics Analysis\n",
    "- Analyze supply chain efficiency and logistics\n",
    "- Create a line plot of delivery times using matplotlib\n",
    "- Generate a box plot of supplier performance using seaborn\n",
    "- Develop an interactive map of supplier locations using plotly\n",
    "\n",
    "### Task 9: Employee Skill Gap Analysis\n",
    "- Analyze current skill distribution and identify gaps\n",
    "- Create a radar chart of skill distribution using matplotlib\n",
    "- Generate a clustered heatmap of skill similarities using seaborn\n",
    "- Develop an interactive sunburst chart of skill hierarchies using plotly\n",
    "\n",
    "\n",
    "### **Final Task: Comprehensive Company Health Report**\n",
    "\n",
    "#### **1. Consolidate Analysis Results**\n",
    "FOR THE STATISTICS E.G. MEAN , MAX , MIN AND TRENDS THROUGH DATAFRAMES AND NUMBERS\n",
    "   - Compile all findings from individual tasks into a single master report.\n",
    "   - Highlight key insights, trends, and actionable recommendations for each task.\n",
    "\n",
    "#### **2. Generate Visualizations**\n",
    "   - Combine all individual visualizations into a single report, ensuring clarity and cohesiveness.\n",
    "   - Arrange visuals by category (e.g., Employee Performance, Project Profitability, Financial Trends) for easy navigation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Presentation Preparation**\n",
    "   - Generate a PDF or PowerPoint report with all visuals and key insights using Python libraries like `Matplotlib`, `Seaborn`, and `Plotly` combined with `PDFKit` or `ReportLab`.\n",
    "   - Create a concise presentation summarizing the findings, with a focus on actionable insights and strategic recommendations.\n",
    "   - Include a Q&A section with potential future data-driven steps.\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# **Employee Data** with anomalies and duplicates\n",
    "employee_data = pd.DataFrame({\n",
    "    'EmployeeID': range(1, 101),\n",
    "    'Name': [f'Employee{i}' for i in range(1, 101)],\n",
    "    'Department': np.random.choice(['R&D', 'Engineering', 'Sales', 'HR', 'Finance'], 100),\n",
    "    'Salary': np.random.randint(50000, 150000, 100),\n",
    "    'Performance': np.random.uniform(2, 5, 100),\n",
    "    'YearsOfExperience': np.random.randint(0, 20, 100),\n",
    "    'SkillLevel': np.random.randint(1, 10, 100)\n",
    "})\n",
    "\n",
    "# Introduce anomalies\n",
    "employee_data.loc[5, 'Salary'] = 9999999  # Extremely high salary\n",
    "employee_data.loc[10, 'Performance'] = -5  # Negative performance value\n",
    "\n",
    "# Add duplicates\n",
    "employee_data = pd.concat([employee_data, employee_data.iloc[2:4]], ignore_index=True)\n",
    "\n",
    "# Add missing values\n",
    "employee_data.loc[8, 'Department'] = np.nan\n",
    "employee_data.loc[20, 'SkillLevel'] = np.nan\n",
    "\n",
    "# **Project Data** with anomalies and duplicates\n",
    "project_data = pd.DataFrame({\n",
    "    'ProjectID': range(1, 51),\n",
    "    'ProjectName': [f'Project{i}' for i in range(1, 51)],\n",
    "    'StartDate': pd.date_range(start='2018-01-01', periods=50),\n",
    "    'Duration': np.random.randint(30, 365, 50),\n",
    "    'Budget': np.random.randint(100000, 1000000, 50),\n",
    "    'ActualCost': np.random.randint(80000, 1200000, 50),\n",
    "    'Revenue': np.random.randint(150000, 2000000, 50),\n",
    "    'ProjectManager': np.random.choice(employee_data['Name'], 50)\n",
    "})\n",
    "\n",
    "# Introduce anomalies\n",
    "project_data.loc[3, 'Duration'] = -50  # Negative duration\n",
    "project_data.loc[7, 'Budget'] = 99999999  # Unreasonably high budget\n",
    "\n",
    "# Add duplicates\n",
    "project_data = pd.concat([project_data, project_data.iloc[5:7]], ignore_index=True)\n",
    "\n",
    "# Add missing values\n",
    "project_data.loc[15, 'ProjectManager'] = np.nan\n",
    "\n",
    "# **Financial Data** with anomalies and duplicates\n",
    "start_date = datetime(2018, 1, 1)\n",
    "dates = [start_date + timedelta(days=30 * i) for i in range(60)]\n",
    "financial_data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Revenue': np.random.randint(1000000, 5000000, 60),\n",
    "    'Expenses': np.random.randint(800000, 4000000, 60),\n",
    "    'Profit': np.random.randint(100000, 1000000, 60)\n",
    "})\n",
    "\n",
    "# Introduce anomalies\n",
    "financial_data.loc[10, 'Revenue'] = -1000000  # Negative revenue\n",
    "financial_data.loc[20, 'Expenses'] = 9999999  # Extremely high expense\n",
    "\n",
    "# Add duplicates\n",
    "financial_data = pd.concat([financial_data, financial_data.iloc[0:2]], ignore_index=True)\n",
    "\n",
    "# Add missing values\n",
    "financial_data.loc[5, 'Profit'] = np.nan\n",
    "\n",
    "# **Customer Data** with anomalies and duplicates\n",
    "customer_data = pd.DataFrame({\n",
    "    'CustomerID': range(1, 201),\n",
    "    'CustomerName': [f'Customer{i}' for i in range(1, 201)],\n",
    "    'Satisfaction': np.random.uniform(2, 5, 200),\n",
    "    'ContractValue': np.random.randint(10000, 1000000, 200),\n",
    "    'ContractDuration': np.random.randint(6, 60, 200)\n",
    "})\n",
    "\n",
    "# Introduce anomalies\n",
    "customer_data.loc[3, 'Satisfaction'] = 10  # Satisfaction beyond the valid range\n",
    "customer_data.loc[50, 'ContractValue'] = -5000  # Negative contract value\n",
    "\n",
    "# Add duplicates\n",
    "customer_data = pd.concat([customer_data, customer_data.iloc[5:8]], ignore_index=True)\n",
    "\n",
    "# Add missing values\n",
    "customer_data.loc[12, 'ContractDuration'] = np.nan\n",
    "\n",
    "# **Market Data** with anomalies and duplicates\n",
    "market_data = pd.DataFrame({\n",
    "    'Quarter': pd.date_range(start='2018-01-01', periods=20, freq='QE'),\n",
    "    'MarketSize': np.random.randint(10000000, 50000000, 20),\n",
    "    'MarketShare': np.random.uniform(0.05, 0.3, 20),\n",
    "    'CompetitorShare': np.random.uniform(0.4, 0.7, 20)\n",
    "})\n",
    "\n",
    "# Introduce anomalies\n",
    "market_data.loc[6, 'MarketShare'] = 1.5  # Market share exceeding 100%\n",
    "market_data.loc[12, 'CompetitorShare'] = -0.2  # Negative competitor share\n",
    "\n",
    "# Add duplicates\n",
    "market_data = pd.concat([market_data, market_data.iloc[2:4]], ignore_index=True)\n",
    "\n",
    "# Add missing values\n",
    "market_data.loc[7, 'MarketSize'] = np.nan\n",
    "\n",
    "\n",
    "# For this task, we'll need to create some dummy supply chain data\n",
    "suppliers = ['Supplier A', 'Supplier B', 'Supplier C', 'Supplier D', 'Supplier E']\n",
    "delivery = pd.DataFrame({\n",
    "'Supplier': suppliers,\n",
    "'AverageDeliveryTime': np.random.uniform(1, 100, 5),\n",
    "'DeliveryVariance': np.random.uniform(0, 2, 5)\n",
    "})\n",
    "\n",
    "# Ensure folder structure for each analysis task\n",
    "def create_folder(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "create_folder('data')\n",
    "\n",
    "# Save to CSV\n",
    "employee_data.to_csv('data/employee_data_with_anomalies.csv', index=False)\n",
    "project_data.to_csv('data/project_data_with_anomalies.csv', index=False)\n",
    "financial_data.to_csv('data/financial_data_with_anomalies.csv', index=False)\n",
    "customer_data.to_csv('data/customer_data_with_anomalies.csv', index=False)\n",
    "market_data.to_csv('data/market_data_with_anomalies.csv', index=False)\n",
    "delivery.to_csv('data/delivery_data_with_anomalies.csv', index=False)\n",
    "\n",
    "# Load the datasets\n",
    "# employee_data = pd.read_csv('data/employee_data_with_anomalies.csv')\n",
    "# project_data = pd.read_csv('data/project_data_with_anomalies.csv')\n",
    "# financial_data = pd.read_csv('data/financial_data_with_anomalies.csv')\n",
    "# customer_data = pd.read_csv('data/customer_data_with_anomalies.csv')\n",
    "# market_data = pd.read_csv('data/market_data_with_anomalies.csv')\n",
    "# delivery_data = pd.read_csv('data/delivery_data_with_anomalies.csv')\n",
    "\n",
    "\n",
    "# Create Plot folder for images\n",
    "# create_folder('plots')\n",
    "# plt.savefig('plots/skill_gap_analysis.png')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
